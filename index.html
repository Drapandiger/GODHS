<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments">
  <meta property="og:title" content="Language-Enhanced Mobile Manipulation"/>
  <meta property="og:description" content="A framework for efficient object search using LLMs and hierarchical navigation."/>
  <meta property="og:url" content="https://your-website-url.com"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Language-Enhanced Mobile Manipulation">
  <meta name="twitter:description" content="A framework for efficient object search using LLMs and hierarchical navigation.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Robotics, LLM, Mobile Manipulation, Object Search, SLAM">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments</title>
  
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- CSS Frameworks (Using CDNs for preview to ensure styling works) -->
  <!-- Replace these with your local static/css/ paths if you are working offline -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.4/dist/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <style>
    body {
      font-family: 'Noto Sans', sans-serif;
    }
    .publication-title {
      font-family: 'Google Sans', sans-serif;
    }
    .publication-authors {
      font-family: 'Google Sans', sans-serif;
    }
    .hero-body {
        padding-top: 3rem;
        padding-bottom: 3rem;
    }
    /* Custom CSS to mimic index.css */
    .publication-video {
        position: relative;
        padding-bottom: 56.25%;
        height: 0;
        overflow: hidden;
        max-width: 100%;
    }
    .publication-video iframe {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }
    .results-carousel {
        overflow: hidden;
    }
    .results-carousel .item {
        margin: 5px;
        overflow: hidden;
        border: 1px solid #e5e5e5;
        border-radius: 10px;
        padding: 0;
        font-size: 0;
    }
    .results-carousel video {
        margin: 0;
    }
    .subtitle {
        font-size: 1rem;
        line-height: 1.5;
        margin-top: 10px !important;
        padding: 10px;
    }
    /* Hide video controls as requested, but be careful with UX */
    video::-webkit-media-controls {
      display: none !important;
    }
    video::-webkit-media-controls-enclosure {
      display: none !important;
    }
    
    .teaser .hero-body {
      padding-top: 0;
      padding-bottom: 40px;
    }

    .is-centered {
      justify-content: center;
    }
    
    .eql-cntrb {
        font-size: 0.9em;
        color: #666;
    }
  </style>

  <!-- Scripts -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.4/dist/js/bulma-slider.min.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- <script src="static/js/index.js"></script> -->
  
  <script>
    $(document).ready(function() {
      // Initialize Carousels
      var carousels = bulmaCarousel.attach('.carousel', {
        slidesToScroll: 1,
        slidesToShow: 1,
        pagination: false,
        loop: true,
        autoplay: true,
        autoplaySpeed: 5000,
        pauseOnHover: true,
      });
    });
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.ce.cit.tum.de/en/air/people/liding-zhang-msc/" target="_blank">Liding Zhang</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="mailto:zeqi.li@tum.de" target="_blank">Zeqi Li</a><sup>*</sup>,</span>
                    <span class="author-block">
                      <a href="https://people.epfl.ch/358612?lang=en" target="_blank">Kuanqi Cai</a><sup>*</sup>,</span>
                        <span class="author-block">
                          <a href="https://www.ce.cit.tum.de/en/air/people/qian-huang-m-sc/" target="_blank">Qian Huang</a>,</span>
                            <span class="author-block">
                              <a href="https://www.ce.cit.tum.de/en/air/people/zhenshan-bing-prof-drrernat/" target="_blank">Zhenshan Bing</a>,</span>
                                <span class="author-block">
                                  <a href="https://www.ce.cit.tum.de/en/air/people/prof-dr-ing-habil-alois-knoll/" target="_blank">Alois Knoll</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Technical University of Munich<br></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <!-- Video Link. -->
                    <!-- PDF Link. -->
                    <span class="link-block">
                      <a href="static/pdfs/TargetSearch.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                    <!-- ArXiv Link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2508.20899" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                  <!-- Video Link. -->
                  <span class="link-block">
                    <a href="https://youtu.be/iU6U3HwSC9U" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          <h2 class="title is-3">Motivation</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img src="static/images/moti1.png" alt="Motivation 1"/>
              <h2 class="subtitle has-text-centered">
                Existing robotic systems mainly rely on spatial data for brute-force searches.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/moti2.png" alt="Motivation 2"/>
              <h2 class="subtitle has-text-centered">
                They may have semantic labels, but canâ€™t perform dynamic semantic reasoning.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/moti3.png" alt="Motivation 3"/>
              <h2 class="subtitle has-text-centered">
                In contrast, humans searching for targets focus on semantic spatial relations.
              </h2>
           </div>
           <div class="item">
            <img src="static/images/moti4.png" alt="Motivation 4"/>
            <h2 class="subtitle has-text-centered">
                The search narrows the area with semantic guidance and common sense.
            </h2>
          </div>
        </div>    
      </div>
    </div>
  </div>
</div>
</section>

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Enabling robots to efficiently search for and identify objects in complex, unstructured environments is critical for diverse applications ranging from household assistance to industrial automation. However, traditional scene representations typically capture only static semantics and lack interpretable contextual reasoning, limiting their ability to guide object search in completely unfamiliar settings. To address this challenge, we propose a language-enhanced hierarchical navigation framework that tightly integrates semantic perception and spatial reasoning. Our method, Goal-Oriented Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large language models (LLMs) to infer scene semantics and guide the search process through a multi-level decision hierarchy. Reliability in reasoning is achieved through the use of structured prompts and logical constraints applied at each stage of the hierarchy. For the specific challenges of mobile manipulation, we introduce a heuristic-based motion planner that combines polar angle sorting with distance prioritization to efficiently generate exploration paths. Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our framework, showing that GODHS can locate target objects with higher search efficiency compared to conventional, non-semantic search strategies. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Youtube video -->
<section class="section hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Video</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
           
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/iU6U3HwSC9U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Approach</h2>
        <div class="content has-text-justified has-text-centered">
          <h3 class="title is-4">GODHS Framework</h3>
          <img src="static/images/fig2.png" alt="GODHS Framework" />
          <p class="has-text-left mt-4"><b>Complete Architecture of the GODHS System:</b> First, the user provides the target object through natural language input, which is then processed by a LLM to extract semantic intent. The robot captures geometric data using LiDAR and RGB images via cameras, generating a topological map with room names through the language model. Within the known map, the LLM-driven prioritization ranks rooms based on the likelihood of containing the target object and navigates to them sequentially. Within each room, the robot detects candidate objects through visual object detection, then utilizes the LLM to analyze whether they are carriers and ranks all collected carriers by the probability of containing the target. For each carrier, the LLM plans a hierarchical search strategy: first analyzing geometric features to identify subregions worth searching (top, interior, etc.), then prioritizing and searching these subregions according to probabilities assigned by the LLM. The loop terminates when the LLM verifies target recognition through visual-language grounding of sensor data.</p>
           
          <h3 class="title is-4 mt-6">Model Chain and Prompt Design</h3>
          <img src="static/images/fig3.png" alt="Model Chain and Prompt Design" />
          <p class="has-text-left mt-4"><b>LLM Reasoning Process (Left):</b> Data Cleaning and Correcting ensure the input and output are structurally valid and semantically coherent, while Processing executes the core task with Knowledge as Augmentation and Feedback to guarantee the result.<br>
          <b>Prompt Design (Right):</b> Input embedding receives the input information, output constraints standardize lexico-syntactic results, task instructions describe goal-oriented content, feature description provides constrained descriptions of features, and there are also additional information and examples.</p>
           
          <h3 class="title is-4 mt-6">Pose Dictionary Estimation</h3>
          <img src="static/images/fig4.png" alt="Pose Dictionary Estimation" />
          <p class="has-text-left mt-4"><b>Estimation of Pose Dictionaries between Chassis and End-Effector:</b> To ensure the mobile robot advances sequentially around the carrier while enabling a single chassis (CH) pose to correspond to multiple end-effector (EE) configurations, we generate search trajectories for the chassis and the end-effector separately, and to have the chassis move forward around the carrier while ensuring that the distance traveled by the end of the actuator at each chassis position is minimized. The process involves five sequential stages: (1) extraction of <b>geometric features</b> \( \mathcal{M} \) from point cloud data, (2) generation and selection of <b>EE poses</b> \( \mathcal{P_{EE}} \), (3) fast evaluation and selection of <b>chassis poses</b> \( \mathcal{P_{CH}} \) via geometric solution, (4) validation of the <b>relation between CH and EE</b> \( \mathcal{P_{CH}^{EE}} \) with subsequent inverse kinematics (IK) solving, and (5) sorting of the resulting <b>pose mappings</b> \( \mathcal{P}_{CH}^{EE} \) for sequential execution by polar angle sorting and lexicographical Sorting. </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-three-fifths">
          <h2 class="title is-3 has-text-centered">Experiment</h2>
          <h3 class="title is-4 is-left">Simulation</h3>
          <div id="simulation-carousel" class="carousel results-carousel">
            <!-- Video items 1-14 Restored -->
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/1.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                We used the DARKO robot and tested it in a <i>flat</i> environment, built grid maps for each room using LiDAR and captured semantic information with a camera.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/2.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                For example, we detected bed and wardrobe in this room. Using prompts, the language model identified the room as a bedroom and marked it on the map.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/3.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                After the mobile manipulator scanned the entire scene, the semantic map was generated.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/4.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Input the target orange to search and sort the rooms based on this target. As shown in prompt and sorting results, we prioritize the <i>living room</i> and <i>kitchen</i>.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/5.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                The system first directs the robot to the <i>living room</i>. In the <i>living room</i>, the robot uses its manipulator for initial observation.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/6.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                During this process, the system identifies several objects. The language model classifies the carriers, such as the TV shelf, billiard table, and sofa.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/7.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                It then determines the order in which carriers should be searched.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/8.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Here, we only search the <i>coffee table</i>.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/9.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                The language model predicts that the <i>top</i> of the <i>coffee table</i> is worth checking.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/10.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                The robot inspects the <i>top</i> of the <i>coffee table</i> but finds the target is not there.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/11.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                After failing to locate the target in the <i>living room</i>, it navigates to the <i>kitchen</i>.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/12.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                It confirms carriers like the oven, fridge, and microwave. From the language model, the carriers worth searching are the <i>fridge</i> and the kitchen table.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/13.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Then, the language model decides to prioritize searching <i>inside</i> the <i>fridge</i>.
              </h2>
            </div>
             <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/14.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Ultimately, the target is found <i>inside</i> the <i>fridge</i> in the <i>kitchen</i>, successfully concluding the search process.
              </h2>
            </div>
          </div>
          <p><br></p>
          <h3 class="title is-4">Results</h3> 
          <div id="results-table-carousel" class="carousel results-carousel">
            <div class="item">
             <img src="static/images/tab1.png" alt="Table 1"/>
             <h2 class="subtitle has-text-centered">
              This table compares four search strategies across success rate, search rates and overall search rates. Our method can effectively perform the search tasks while significantly reducing search costs.
             </h2>
           </div>
           <div class="item">
             <img src="static/images/tab2.png" alt="Table 2"/>
             <h2 class="subtitle has-text-centered">
              We evaluate three key performance metrics: the EE Path Length, the CH Path Length, and the Execution Time Ratio.
            </h2>
          </div>
       </div>    
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <div class="columns is-centered">
      <div class="column is-three-fifths">
    <pre><code>@misc{target2025search,
  title={Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments}, 
  author={Liding Zhang and Zeqi Li and Kuanqi Cai and Qian Huang and Zhenshan Bing and Alois Knoll},
  year={2025},
  eprint={2508.20899},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2508.20899}, 
}</code></pre>
  </div>
</div>
  </div>
</section>

</body>
</html>
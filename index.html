<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=AMFFKhkAAAAJ&hl=en" target="_blank">Liding Zhang</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="zeqi.li@tum.de" target="_blank">Zeqi Li</a><sup>*</sup>,</span>
                    <span class="author-block">
                      <a href="https://scholar.google.com/citations?user=3Y9wVfMOtP4C&hl=en" target="_blank">Kuanqi Cai</a><sup>*</sup>,</span>
                        <span class="author-block">
                          <a href="https://www.ce.cit.tum.de/air/people/zhenshan-bing-drrernat/" target="_blank">Zhenshan Bing</a>,</span>
                            <span class="author-block">
                              <a href="https://www.ce.cit.tum.de/en/air/people/prof-dr-ing-habil-alois-knoll/" target="_blank">Alois Knoll</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Technical University of Munich<br></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">


                    <!-- Video Link. -->
                    <span class="link-block">
                      <a href="https://youtu.be/e0FzMS5M7dw" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>


 
                <!-- ArXiv abstract Link -->
<!--
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
-->              

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          <h2 class="title is-3">Motivation</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/moti1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Existing robotic systems mainly rely on spatial data for brute-force searches.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/moti2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          They may have semantic labels, but can’t perform dynamic semantic reasoning.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/moti3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          In contrast, humans searching for targets focus on semantic spatial relation.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/moti4.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
          The searches narrows the area with semantic guidances and common sense.
      </h2>
    </div>
  </div>    
</div>
</div>
</div>
</div>
</section>

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Enabling robots to efficiently search for and identify objects in complex, unstructured environments is critical for diverse applications ranging from household assistance to industrial automation. However, traditional scene representations typically capture only static semantics and lack interpretable contextual reasoning, limiting their ability to guide object search in completely unfamiliar settings. To address this challenge, we propose a language-enhanced hierarchical navigation framework that tightly integrates semantic perception and spatial reasoning. Our method, Goal-Oriented Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large language models (LLMs) to infer scene semantics and guide the search process through a multi-level decision hierarchy. Reliability in reasoning is achieved through the use of structured prompts and logical constraints applied at each stage of the hierarchy. For the specific challenges of mobile manipulation, we introduce a heuristic-based motion planner that combines polar angle sorting with distance prioritization to efficiently generate exploration paths. Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our framework, showing that GODHS can locate target objects with higher search efficiency compared to conventional, non-semantic search strategies. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Youtube video -->
<section class="section hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Video</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/e0FzMS5M7dw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Approach</h2>
        <div class="content has-text-justified has-text-centered">
          <h3 class="title is-4">GODHS Framework</h3>
          <img src="static/images/fig2.png" alt="GODHS Framework" />
          <p><b>Complete Architecture of the GODHS System:</b> First, the user provides the target object through natural language input, which is then processed by a LLM to extract semantic intent. The robot captures geometric data using LiDAR and RGB images via cameras, generating a topological map with room names through the language model. Within the known map, the LLM-driven prioritization ranks rooms based on the likelihood of containing the target object and navigates to them sequentially. Within each room, the robot detects candidate objects through visual object detection, then utilizes the LLM to analyze whether they are carriers and ranks all collected carriers by the probability of containing the target. For each carrier, the LLM plans a hierarchical search strategy: first analyzing geometric features to identify subregions worth searching (top, interior, etc.), then prioritizing and searching these subregions according to probabilities assigned by the LLM. The loop terminates when the LLM verifies target recognition through visual-language grounding of sensor data.</p>
          
          <h3 class="title is-4">Model Chain and Prompt Design</h3>
          <img src="static/images/fig3.png" alt="Model Chain and Prompt Design" />
          <p><b>LLM Reasoning Process (Left):</b> Data Cleaning and Correcting ensure the input and output are structurally valid and semantically coherent, while Processing executes the core task with Knowledge as Augmentation and Feedback to guarantee the result.<br>
          <b>Prompt Design (Right):</b> Input embedding receives the input information, output constraints standardize lexico-syntactic results, task instructions describe goal-oriented content, feature description provides constrained descriptions of features, and there are also additional information and examples.</p>
          
          <h3 class="title is-4">Pose Dictionary Estimation</h3>
          <img src="static/images/fig4.png" alt="Pose Dictionary Estimation" />
          <p><b>Estimation of Pose Dictionaries between Chassis and End-Effector:</b> To ensure the mobile robot advances sequentially around the carrier while enabling a single chassis (CH) pose to correspond to multiple end-effector (EE) configurations, we generate search trajectories for the chassis and the end-effector separately, and to have the chassis move forward around the carrier while ensuring that the distance traveled by the end of the actuator at each chassis position is minimized. The process involves five sequential stages: (1) extraction of <b>geometric features</b> \( \mathcal{M} \) from point cloud data, (2) generation and selection of <b>EE poses</b> \( \mathcal{P_{EE}} \), (3) fast evaluation and selection of <b>chassis poses</b> \( \mathcal{P_{CH}} \) via geometric solution, (4) validation of the <b>relation between CH and EE</b> \( \mathcal{P_{CH}^{EE}} \) with subsequent inverse kinematics (IK) solving, and (5) sorting of the resulting <b>pose mappings</b> \( \mathcal{P}_{CH}^{EE} \) for sequential execution by polar angle sorting and lexicographical Sorting. </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-three-fifths">
          <h2 class="title is-3 has-text-centered">Experiment</h2>
          <h3 class="title is-4 is-left">Simulation</h3>
          <div id="results-carousel" class="carousel results-carousel">
            <!-- Video items 1-14 -->
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/1.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                We used the DARKO robot and tested it in a <i>flat</i> environment, built grid maps for each room using LiDAR and captured semantic information with a camera.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/2.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                For example, we detected bed and wardrobe in this room. Using prompts, the language model identified the room as a bedroom and marked it on the map.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/3.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                After the mobile manipulator scanned the entire scene, the semantic map was generated.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/4.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Input the target orange to search and sort the rooms based on this target. As shown in prompt and sorting results, we prioritize the <i>living room</i> and <i>kitchen</i>.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/5.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                The system first directs the robot to the <i>living room</i>. In the <i>living room</i>, the robot uses its manipulator for initial observation.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/6.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                During this process, the system identifies several objects. The language model classifies the carriers, such as the TV shelf, billiard table, and sofa.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/7.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                It then determines the order in which carriers should be searched. 
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/8.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Here, we only search the <i>coffee table</i>.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/9.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                The language model predicts that the <i>top</i> of the <i>coffee table</i> is worth checking.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/10.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                The robot inspects the <i>top</i> of the <i>coffee table</i> but finds the target is not there.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/11.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                After failing to locate the target in the <i>living room</i>, it navigates to the <i>kitchen</i>.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/12.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                It confirms carriers like the oven, fridge, and microwave. From the language model, the carriers worth searching are the <i>fridge</i> and the kitchen table.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/13.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Then, the language model decides to prioritize searching <i>inside</i> the <i>fridge</i>.
              </h2>
            </div>
            <div class="item">
              <video muted autoplay loop playsinline style="width: 100%; height: auto">
                <source src="static/videos/14.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Ultimately, the target is found <i>inside</i> the <i>fridge</i> in the <i>kitchen</i>, successfully concluding the search process.
              </h2>
            </div>
          </div>
          <p><br></p>
          <h3 class="title is-4">Results</h3> 
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
             <!-- Your image here -->
             <img src="static/images/tab1.png" alt="MY ALT TEXT"/>
             <h2 class="subtitle has-text-centered">
              This table compares four search strategies across success rate, search rates and overall search rates. our method can effectively perform the search tasks while significantly reducing search costs, thereby validating the feasibility of GODHS. However, the experimental results demonstrate that the search success rate progressively decreases following semantic analysis. Traditional methods fail in narrow spaces due to grid-based maps being unable to model 3D semantic relationships, while GODHS faces additional limitations from generative language models, particularly the symbol grounding problem.
             </h2>
           </div>
           <div class="item">
             <!-- Your image here -->
             <img src="static/images/tab2.png" alt="MY ALT TEXT"/>
             <h2 class="subtitle has-text-centered">
              We evaluate three key performance metrics: the EE Path Length, which represents the ratio of the total EE travel distance to the theoretical shortest EE path; the CH Path Length, defined similarly for the CH poses; and the Execution Time Ratio, which is the ratio of execution time with a lower value indicating better efficiency. Table presents the comparative results, demonstrating that lexicographical sorting effectively reduces EE path length while polar angle sorting significantly improves CH path efficiency. When both strategies are applied, the method achieves the highest optimization in both path length and execution time, confirming its effectiveness in improving motion planning.
            </h2>
          </div>
       </div>    
        </div>
      </div>
    </div>
  </div>
</section>

<style>
/* 隐藏控制条 */
video::-webkit-media-controls {
  display: none !important;
}
video::-webkit-media-controls-enclosure {
  display: none !important;
}
</style>

<section class="section" id="BibTeX">
  <div class="container">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <div class="columns is-centered">
      <div class="column is-three-fifths">
    <pre><code>Coming soon</code></pre>
  </div>
</div>
  </div>
</section>



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>


